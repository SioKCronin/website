+++
title = ""
date = "2018-04-24T11:53:47-07:00"
draft = false

+++
### Upcoming
* 7.5.2018 --- The Poetics of Swarm Intelligence @ La Casa Naranja
(SF)
* 7 DEC 2018 --- This Fountain of Being @ Center for New Music (SF)

### Past

* 10.11.2017 --- Step by Step @ Metis (SF)<br/>
* 9.21.2016 --- Forms @ Conference on Complex Systems (Amsterdam)<br/>
* 8.9.2016 --- Form and Movement in Complex Systems @ PyLadies (SF)<br/>
* 6.4.2016 --- On People, Cubes and Snacks @ AlterConf (SF)

</br>

### The Poetics of Swarm Intelligence

Swarm intelligence is the collective behavior of decentralized,
self-organized systems. The whole being other than the sum of its parts.
Many human traditions have described swarm intelligence, and while my
research is becoming more and more rooted in finding real-world
applications of swarm intelligence algorithms from the evolutionary
computing tradition, my heritage as an empath and artists makes it
impossible for me not to be porous to this broader heritage. This talk
is rooted in my ongoing work on PSO-baselines, an open source repository
of particle swarm optimization algorithms implemented in Python, and in
my broader commitment to reconnecting with traditions of collective
ideation and collaboration. Together we will explore how thinking as and
with the collective avails us to dynamic modes of problem solving.

### Step by Step: When and How To Use Stochastic Optimization

You’ve got your model up and running, and now you’d like to tune your
hyperparameters. If you’ve gotten to this point in the process, it means
you’ve successfully cleaned your data, engineered your features, coded
up your model, identified your objective function, and are now looking
to optimize. You could cancel your evening plans to manually explore all
hyperparameter combinations, or programmatically iterate through them
with something like grid search, but you want to try something else.
Something that will produce impressive results in a fraction of the
time. Enter stochastic optimization.

This presentation will build on what you already know about parameter
optimization to dive deeper into the world of stochastic optimization.
In particular, we’ll explore stochastic gradient descent, simulated
annealing, and particle swarm optimization - cultivating intuition on
how these algorithms work, when they can be applied, what their
underlying topologies look like, and how you can get the best
performance out of them. If possible, bring models you’re working, so
you can relate the content directly to what you’re building. I’ll be
available afterwards (and by e-mail) to ensure you have what you need to
implement the optimizers we discuss. Code examples will be presented in
Python, but I’ll also provide libraries and resources in R.

