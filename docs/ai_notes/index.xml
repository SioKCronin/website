<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ai_notes on Siobhan K Cronin</title>
    <link>https://siokcronin.github.io/website/ai_notes/</link>
    <description>Recent content in Ai_notes on Siobhan K Cronin</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    
	<atom:link href="https://siokcronin.github.io/website/ai_notes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>https://siokcronin.github.io/website/ai_notes/lecture_notes/deep_rl_bootcamp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://siokcronin.github.io/website/ai_notes/lecture_notes/deep_rl_bootcamp/</guid>
      <description> Deep RL Bootcamp 1 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://siokcronin.github.io/website/ai_notes/math/linear_algebra_basics_with_scipy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://siokcronin.github.io/website/ai_notes/math/linear_algebra_basics_with_scipy/</guid>
      <description>Basics with Scipy import scipy.linalg as la import numpy as np  Motivating example We are entering three pies into the bakeoff, and want to make sure we win. We&amp;rsquo;ve tried a lot of different ingredients, but ultimatley have found that taste comes down to butter, salt, sugar, and grandma&amp;rsquo;s secret spice blend.
w = 1 #tbl_of_butter x = 1 #dash_of_salt y = 1 #spoonful_of_sugar z = 1 #undisclosed_measurement_of_spice pie1 = 3*w + 5*x + 30*y + 10*z pie2 = 2*w + 3*x + 40*y + 15*z pie3 = 4*w + 2*x + 10*y + 30*z pie4 = 10*w + 2*x + 10*y + 30*z  A = np.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://siokcronin.github.io/website/ai_notes/reading_notes/reinforcement_learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://siokcronin.github.io/website/ai_notes/reading_notes/reinforcement_learning/</guid>
      <description>Reinforcement Learning Reinforcement Learning: An Introduction (1998). Richard Sutton &amp;amp; Andrew Barto.
At the onset, I&amp;rsquo;m curious to know how much has changed since this book&amp;rsquo;s publication 20 years ago. That being said, as these are two leaders in the field, I&amp;rsquo;m interested in gaining a sense of their perspective on the history/origin of this subfield, and acclimating to some of the core concepts/constructs.
Chapter 1 One of the key takeaways from this chapter was the distinction between the value function and rewards function in an RL problem.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://siokcronin.github.io/website/ai_notes/reinforcement_learning/ddpg/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://siokcronin.github.io/website/ai_notes/reinforcement_learning/ddpg/</guid>
      <description> Deep Deterministic Policy Gradients (DDPG) For this model-free RL algorithm for continuous spaces, episodes are generated using a behavioral polcy, which is a noisy version of the target policy. There are two neural networks, an actor and a critic, where the targets for the critic are the actions outputted by the actor. Actor is trained using mini-batch GD on the inverse expected Q value.
Algorithm algorithm = some_off_policy_RL_algorithm initialize replay buffer M = 10 # Number of episodes for epsisode in range(M):  </description>
    </item>
    
    <item>
      <title></title>
      <link>https://siokcronin.github.io/website/ai_notes/reinforcement_learning/dqn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://siokcronin.github.io/website/ai_notes/reinforcement_learning/dqn/</guid>
      <description>Deep Q-Networks (DQN) This model-free RL algorithm, is like traditional q-learning, only we maintain neural network that approximates $Q*$.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://siokcronin.github.io/website/ai_notes/reinforcement_learning/her/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://siokcronin.github.io/website/ai_notes/reinforcement_learning/her/</guid>
      <description> Hindsight Experience Replay (HER) This technique has a super clever way of dealing with sparse reward situations. Essentially, it calls misses successes, so that learning can be made even when we miss our target. It can be combined with other off-policy RL algorithms, and I show how to do that in other posts.
Algorithm algorithm = some_off_policy_RL_algorithm initialize replay buffer M = 10 # Number of episodes for epsisode in range(M): goal, s0 = sampler for t in range(T):  </description>
    </item>
    
    <item>
      <title></title>
      <link>https://siokcronin.github.io/website/ai_notes/reinforcement_learning/inventory_control_theory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://siokcronin.github.io/website/ai_notes/reinforcement_learning/inventory_control_theory/</guid>
      <description> Inventory Control Theory  Just in Time Levalized Production Inventory Depletion Hadley-Within model AHM-model class, lot-size model buy-ahead model Wagner-Within model  </description>
    </item>
    
    <item>
      <title></title>
      <link>https://siokcronin.github.io/website/ai_notes/reinforcement_learning/lstm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://siokcronin.github.io/website/ai_notes/reinforcement_learning/lstm/</guid>
      <description>LSTM Recurrent Neural Network (RNN) RNNs are networks with loops so that information can persist.
Long Short-Term Memory Long Short-Term Memory (LSTM) networks are a specific type of RNN capable of learning long-term dependencies.
Key steps are a forget gate layer and input gate layer.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://siokcronin.github.io/website/ai_notes/reinforcement_learning/monte_carlo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://siokcronin.github.io/website/ai_notes/reinforcement_learning/monte_carlo/</guid>
      <description> Monte Carlo Examples when Monte Carlo is preferred to TD </description>
    </item>
    
    <item>
      <title></title>
      <link>https://siokcronin.github.io/website/ai_notes/reinforcement_learning/td/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://siokcronin.github.io/website/ai_notes/reinforcement_learning/td/</guid>
      <description> Temporal Difference (TD) Learning Examples when more advantegeous than Monte Carlo  When there is variability early on  </description>
    </item>
    
    <item>
      <title></title>
      <link>https://siokcronin.github.io/website/ai_notes/software_engineering/hashing_with_python/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://siokcronin.github.io/website/ai_notes/software_engineering/hashing_with_python/</guid>
      <description>Hashing Strings with Python Hash functions map sequences of bytes to fixed length sequences. The value returned by a hash function is often called a hash, message digest, hash value, or checksum.
MD5 binascii If you&amp;rsquo;re looking to hash a string, you&amp;rsquo;ll need to convert your string to binary.
# Convert string to binary binary_string = b&#39;abcd&#39; print(binary_string)  b&#39;abcd&#39;  # Generate hexadecimal from binary string import binascii binascii.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://siokcronin.github.io/website/ai_notes/swarm_intelligence/prob/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://siokcronin.github.io/website/ai_notes/swarm_intelligence/prob/</guid>
      <description>Prob \begin{align} \dot{x} &amp;amp; = \sigma(y-x) \dot{y} &amp;amp; = \rho x - y - xz \dot{z} &amp;amp; = -\beta z + xy \end{align}
$\alpha = 1000$
import matplotlib import numpy as np import matplotlib.pyplot as plt %matplotlib inline  /usr/local/lib/python3.6/site-packages/matplotlib/font_manager.py:278: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment. &#39;Matplotlib is building the font cache using fc-list. &#39;  x = np.linspace(0, 3*np.pi, 500) plt.</description>
    </item>
    
  </channel>
</rss>