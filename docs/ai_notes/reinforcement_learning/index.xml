<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Reinforcement Learning on Siobhán K Cronin</title>
    <link>http://siobhankcronin.com/ai_notes/reinforcement_learning/</link>
    <description>Recent content in Reinforcement Learning on Siobhán K Cronin</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="http://siobhankcronin.com/ai_notes/reinforcement_learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>http://siobhankcronin.com/ai_notes/reinforcement_learning/ddpg/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://siobhankcronin.com/ai_notes/reinforcement_learning/ddpg/</guid>
      <description> Deep Deterministic Policy Gradients (DDPG) For this model-free RL algorithm for continuous spaces, episodes are generated using a behavioral polcy, which is a noisy version of the target policy. There are two neural networks, an actor and a critic, where the targets for the critic are the actions outputted by the actor. Actor is trained using mini-batch GD on the inverse expected Q value.
Algorithm algorithm = some_off_policy_RL_algorithm initialize replay buffer M = 10 # Number of episodes for epsisode in range(M):  </description>
    </item>
    
    <item>
      <title></title>
      <link>http://siobhankcronin.com/ai_notes/reinforcement_learning/inventory_control_theory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://siobhankcronin.com/ai_notes/reinforcement_learning/inventory_control_theory/</guid>
      <description> Inventory Control Theory  Just in Time Levalized Production Inventory Depletion Hadley-Within model AHM-model class, lot-size model buy-ahead model Wagner-Within model  </description>
    </item>
    
    <item>
      <title></title>
      <link>http://siobhankcronin.com/ai_notes/reinforcement_learning/monte_carlo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://siobhankcronin.com/ai_notes/reinforcement_learning/monte_carlo/</guid>
      <description> Monte Carlo Examples when Monte Carlo is preferred to TD  </description>
    </item>
    
    <item>
      <title></title>
      <link>http://siobhankcronin.com/ai_notes/reinforcement_learning/td/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://siobhankcronin.com/ai_notes/reinforcement_learning/td/</guid>
      <description> Temporal Difference (TD) Learning Examples when more advantegeous than Monte Carlo  When there is variability early on   </description>
    </item>
    
    <item>
      <title>Hindsight Experience Replay (HER)</title>
      <link>http://siobhankcronin.com/ai_notes/reinforcement_learning/her/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://siobhankcronin.com/ai_notes/reinforcement_learning/her/</guid>
      <description> This technique has a super clever way of dealing with sparse reward situations. Essentially, it calls misses successes, so that learning can be made even when we miss our target. It can be combined with other off-policy RL algorithms, and I show how to do that in other posts.
Algorithm algorithm = some_off_policy_RL_algorithm initialize replay buffer M = 10 # Number of episodes for epsisode in range(M): goal, s0 = sampler for t in range(T):  </description>
    </item>
    
    <item>
      <title>Long Short Term Memory (LSTM)</title>
      <link>http://siobhankcronin.com/ai_notes/reinforcement_learning/lstm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://siobhankcronin.com/ai_notes/reinforcement_learning/lstm/</guid>
      <description>Grid Long Short-Term Memory (LSTM) Long Short-Term Memory (LSTM) networks are networks of LSTM cells arranged in a grid to facilitate deep and sequential computation. LSTM networks are RNN architectures with an improved ability to store and access memory. The magic is a a gating mechansim that controls access to memory cells. The gating helps preserve signal for longer, and also propogates error for longer than typical RNN structures. The gating helps focus the network on specfic aspects of the input signal, while ignoring other parts.</description>
    </item>
    
    <item>
      <title>Proximal Policy Optimization (PPO)</title>
      <link>http://siobhankcronin.com/ai_notes/reinforcement_learning/ppo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://siobhankcronin.com/ai_notes/reinforcement_learning/ppo/</guid>
      <description>As I continue spelunking into the PSO cave in hopes of finding an algorithm design strain of gold I can follow, I&amp;rsquo;m getting more and more excited about the RL research at OpenAI. In particular, I perked my ears up when I read that Proximal Policy Optimzation became the default RL algorithm at OpenAI last summer. Let&amp;rsquo;s find out why.
First off, what is it PPO?
The stage is set by considering policy gradient methods (PG) in using deep neural nets in control problems.</description>
    </item>
    
  </channel>
</rss>